{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a42c83b1",
   "metadata": {},
   "source": [
    "# AI Text Detector - Model Comparison\n",
    "\n",
    "This notebook compares the performance of RoBERTa and DistilBERT models for AI text detection.\n",
    "\n",
    "## Models Compared\n",
    "1. **RoBERTa-base**: High accuracy, slower inference\n",
    "2. **DistilBERT-base**: Fast inference, smaller size\n",
    "\n",
    "## Comparison Metrics\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Training time and inference speed\n",
    "- Model size and memory usage\n",
    "- ROC curves and performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cad766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model results\n",
    "try:\n",
    "    with open('../models/roberta_results.json', 'r') as f:\n",
    "        roberta_results = json.load(f)\n",
    "    print(\"RoBERTa results loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"RoBERTa results not found. Please run the RoBERTa training notebook first.\")\n",
    "    roberta_results = None\n",
    "\n",
    "try:\n",
    "    with open('../models/distilbert_results.json', 'r') as f:\n",
    "        distilbert_results = json.load(f)\n",
    "    print(\"DistilBERT results loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"DistilBERT results not found. Please run the DistilBERT training notebook first.\")\n",
    "    distilbert_results = None\n",
    "\n",
    "if roberta_results and distilbert_results:\n",
    "    print(\"\\nBoth model results loaded. Proceeding with comparison...\")\n",
    "else:\n",
    "    print(\"\\nPlease ensure both models are trained before running this comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c59dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "if roberta_results and distilbert_results:\n",
    "    comparison_data = {\n",
    "        'Model': ['RoBERTa-base', 'DistilBERT-base'],\n",
    "        'Accuracy': [roberta_results['test_accuracy'], distilbert_results['test_accuracy']],\n",
    "        'F1-Score': [roberta_results['test_f1'], distilbert_results['test_f1']],\n",
    "        'Precision': [roberta_results['test_precision'], distilbert_results['test_precision']],\n",
    "        'Recall': [roberta_results['test_recall'], distilbert_results['test_recall']],\n",
    "        'Training Time (min)': [\n",
    "            roberta_results.get('training_time_minutes', 'N/A'),\n",
    "            distilbert_results.get('training_time_minutes', 'N/A')\n",
    "        ],\n",
    "        'Inference Time (ms)': [\n",
    "            roberta_results.get('inference_time_ms', 'N/A'),\n",
    "            distilbert_results.get('inference_time_ms', 'N/A')\n",
    "        ],\n",
    "        'Model Parameters': [\n",
    "            roberta_results.get('model_parameters', 'N/A'),\n",
    "            distilbert_results.get('model_parameters', 'N/A')\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"Model Comparison Table:\")\n",
    "    print(comparison_df.round(4))\n",
    "else:\n",
    "    print(\"Cannot create comparison without both model results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance metrics\n",
    "if roberta_results and distilbert_results:\n",
    "    metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "    roberta_scores = [comparison_df.loc[0, metric] for metric in metrics]\n",
    "    distilbert_scores = [comparison_df.loc[1, metric] for metric in metrics]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Performance Metrics Comparison', 'Model Size Comparison',\n",
    "                       'Speed Comparison', 'Accuracy vs Speed Trade-off'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Performance metrics\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=metrics, y=roberta_scores, name='RoBERTa', marker_color='#FF6B6B'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=metrics, y=distilbert_scores, name='DistilBERT', marker_color='#4ECDC4'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Model size comparison\n",
    "    if isinstance(roberta_results.get('model_parameters'), (int, float)) and isinstance(distilbert_results.get('model_parameters'), (int, float)):\n",
    "        model_sizes = [roberta_results['model_parameters']/1e6, distilbert_results['model_parameters']/1e6]\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=['RoBERTa', 'DistilBERT'], y=model_sizes, \n",
    "                  marker_color=['#FF6B6B', '#4ECDC4'], showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Speed comparison\n",
    "    if isinstance(roberta_results.get('inference_time_ms'), (int, float)) and isinstance(distilbert_results.get('inference_time_ms'), (int, float)):\n",
    "        inference_times = [roberta_results['inference_time_ms'], distilbert_results['inference_time_ms']]\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=['RoBERTa', 'DistilBERT'], y=inference_times, \n",
    "                  marker_color=['#FF6B6B', '#4ECDC4'], showlegend=False),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Accuracy vs Speed trade-off\n",
    "    if isinstance(roberta_results.get('inference_time_ms'), (int, float)) and isinstance(distilbert_results.get('inference_time_ms'), (int, float)):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[roberta_results['inference_time_ms'], distilbert_results['inference_time_ms']],\n",
    "                y=[roberta_results['test_accuracy'], distilbert_results['test_accuracy']],\n",
    "                mode='markers+text',\n",
    "                text=['RoBERTa', 'DistilBERT'],\n",
    "                textposition=\"top center\",\n",
    "                marker=dict(size=15, color=['#FF6B6B', '#4ECDC4']),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text=\"Metrics\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Model\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Parameters (Millions)\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Model\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Inference Time (ms)\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Inference Time (ms)\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(title_text=\"Model Performance Comparison\", height=800)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Cannot create visualizations without both model results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe1d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confusion matrices\n",
    "if roberta_results and distilbert_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # RoBERTa confusion matrix\n",
    "    roberta_cm = np.array(roberta_results['confusion_matrix'])\n",
    "    sns.heatmap(roberta_cm, annot=True, fmt='d', cmap='Reds', \n",
    "                xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'],\n",
    "                ax=axes[0])\n",
    "    axes[0].set_title('RoBERTa Confusion Matrix')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # DistilBERT confusion matrix\n",
    "    distilbert_cm = np.array(distilbert_results['confusion_matrix'])\n",
    "    sns.heatmap(distilbert_cm, annot=True, fmt='d', cmap='Greens', \n",
    "                xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'],\n",
    "                ax=axes[1])\n",
    "    axes[1].set_title('DistilBERT Confusion Matrix')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate error analysis\n",
    "    roberta_errors = roberta_cm[0,1] + roberta_cm[1,0]  # False positives + False negatives\n",
    "    distilbert_errors = distilbert_cm[0,1] + distilbert_cm[1,0]\n",
    "    \n",
    "    print(f\"\\nError Analysis:\")\n",
    "    print(f\"RoBERTa total errors: {roberta_errors}\")\n",
    "    print(f\"DistilBERT total errors: {distilbert_errors}\")\n",
    "    print(f\"Difference: {abs(roberta_errors - distilbert_errors)} errors\")\n",
    "else:\n",
    "    print(\"Cannot create confusion matrix comparison without both model results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092825a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary and recommendations\n",
    "if roberta_results and distilbert_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Determine best performing model\n",
    "    if roberta_results['test_accuracy'] > distilbert_results['test_accuracy']:\n",
    "        best_accuracy = \"RoBERTa\"\n",
    "        accuracy_diff = roberta_results['test_accuracy'] - distilbert_results['test_accuracy']\n",
    "    else:\n",
    "        best_accuracy = \"DistilBERT\"\n",
    "        accuracy_diff = distilbert_results['test_accuracy'] - roberta_results['test_accuracy']\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ ACCURACY WINNER: {best_accuracy}\")\n",
    "    print(f\"   Accuracy difference: {accuracy_diff:.4f} ({accuracy_diff*100:.2f}%)\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    if isinstance(roberta_results.get('inference_time_ms'), (int, float)) and isinstance(distilbert_results.get('inference_time_ms'), (int, float)):\n",
    "        if roberta_results['inference_time_ms'] < distilbert_results['inference_time_ms']:\n",
    "            speed_winner = \"RoBERTa\"\n",
    "            speed_diff = distilbert_results['inference_time_ms'] - roberta_results['inference_time_ms']\n",
    "        else:\n",
    "            speed_winner = \"DistilBERT\"\n",
    "            speed_diff = roberta_results['inference_time_ms'] - distilbert_results['inference_time_ms']\n",
    "        \n",
    "        print(f\"\\nâš¡ SPEED WINNER: {speed_winner}\")\n",
    "        print(f\"   Speed advantage: {speed_diff:.1f}ms faster per prediction\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š DETAILED METRICS:\")\n",
    "    print(f\"   RoBERTa    - Accuracy: {roberta_results['test_accuracy']:.4f}, F1: {roberta_results['test_f1']:.4f}\")\n",
    "    print(f\"   DistilBERT - Accuracy: {distilbert_results['test_accuracy']:.4f}, F1: {distilbert_results['test_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    print(f\"   ðŸ“ˆ For HIGHEST ACCURACY: Use RoBERTa\")\n",
    "    print(f\"      - Best for research, batch processing\")\n",
    "    print(f\"      - When accuracy is more important than speed\")\n",
    "    \n",
    "    print(f\"   âš¡ For REAL-TIME APPLICATIONS: Use DistilBERT\")\n",
    "    print(f\"      - Best for web apps, mobile apps\")\n",
    "    print(f\"      - When speed matters and accuracy is sufficient\")\n",
    "    \n",
    "    print(f\"   ðŸŽ¯ For BALANCED PERFORMANCE: DistilBERT\")\n",
    "    print(f\"      - Good accuracy with much better speed\")\n",
    "    print(f\"      - Recommended for most production use cases\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"Cannot generate summary without both model results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "if roberta_results and distilbert_results:\n",
    "    comparison_summary = {\n",
    "        'comparison_date': pd.Timestamp.now().isoformat(),\n",
    "        'models_compared': ['RoBERTa-base', 'DistilBERT-base'],\n",
    "        'dataset_info': {\n",
    "            'total_samples': roberta_results['training_samples'] + roberta_results['test_samples'],\n",
    "            'training_samples': roberta_results['training_samples'],\n",
    "            'test_samples': roberta_results['test_samples']\n",
    "        },\n",
    "        'performance_comparison': comparison_data,\n",
    "        'recommendations': {\n",
    "            'best_accuracy': best_accuracy,\n",
    "            'best_speed': speed_winner if 'speed_winner' in locals() else 'Unknown',\n",
    "            'recommended_for_production': 'DistilBERT',\n",
    "            'recommended_for_research': 'RoBERTa'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('../models/model_comparison.json', 'w') as f:\n",
    "        json.dump(comparison_summary, f, indent=2)\n",
    "    \n",
    "    print(\"Comparison results saved to '../models/model_comparison.json'\")\n",
    "    \n",
    "    # Also save as CSV for easy viewing\n",
    "    comparison_df.to_csv('../models/model_comparison.csv', index=False)\n",
    "    print(\"Comparison table saved to '../models/model_comparison.csv'\")\n",
    "    \n",
    "    print(\"\\nModel comparison completed successfully!\")\n",
    "    print(\"Both models are ready for deployment in the Streamlit app.\")\n",
    "else:\n",
    "    print(\"Cannot save comparison without both model results.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
